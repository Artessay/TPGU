{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tf = tensorflow.compat.v1\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "tf.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "valid_ratio = 0.2\n",
    "\n",
    "input_size = 202\n",
    "num_neurons = 160\n",
    "output_size = 158\n",
    "\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.95\n",
    "\n",
    "max_epoch = 50\n",
    "batch_size = 1\n",
    "\n",
    "save_log_iter = 10\n",
    "display_iter = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集处理类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoilerDataSet(object):\n",
    "    \"\"\"\n",
    "    first run data_preparation.py to generate data.csv\n",
    "    prepare boiler training and validation dataset\n",
    "    simple version(small action dimension)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_steps, val_ratio=0.1):\n",
    "        self.num_steps = num_steps  # 历史步长\n",
    "        self.val_ratio = val_ratio  # 训练集与测试集比例\n",
    "        \n",
    "        # Read csv file\n",
    "        self.raw_data = pd.read_csv(\"./Simulator/data/sim_train.csv\", index_col='时间戳')\n",
    "\n",
    "        # sort csv file\n",
    "        cols = self.raw_data.columns.tolist()\n",
    "        # print(\"origin len: {0}\".format(len(cols)))\n",
    "        cols = (cols[51:52] + cols[53:59] + cols [60:61] + cols[62:63] + cols[150:152]   # external input \n",
    "            + cols[0:50] + cols[52:53] + cols[122:139]  # Coal Pulverizing state\n",
    "            + cols[50:51] + cols[59:60] + cols[61:62] + cols[63:101] + cols[112:114] + cols[118:122] + cols[139:145] + cols[146:149] + cols[152:158]    # Burning state\n",
    "            + cols[101:112] + cols[114:118] + cols[145:146] + cols[149:150] # Steam Circulation state\n",
    "            + cols[158:173] + cols[196:202] # Coal Pulverizing action\n",
    "            + cols[173:192]                 # Burning action\n",
    "            + cols[192:196])                # Steam Circulation action\n",
    "        print(\"ordered len: {0}\".format(len(cols)))\n",
    "        # self.raw_data = self.raw_data[cols]\n",
    "\n",
    "        # 划分训练集和测试集\n",
    "        self.train_X, self.train_y, self.valid_X, self.valid_y = self.prepare_data(self.raw_data)\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        # split into groups of num_steps\n",
    "\n",
    "        # 取出输入数据，学习num_steps步长的历史，iloc：通过行号获取行数据\n",
    "        X = np.array([data.iloc[i: i + self.num_steps].values\n",
    "                    for i in range(len(data) - self.num_steps)])\n",
    "\n",
    "        # 取出输出数据，预测第num_steps步的值训练，ix / loc 可以通过行号和行标签进行索引\n",
    "        # 这里只要对状态量进行预测即可，0-157列为 'A磨煤机电流':'大渣可燃物含量'\n",
    "        y = np.array([data.iloc[i+1: i + self.num_steps+1, 0:158].values\n",
    "                    for i in range(len(data) - self.num_steps)])\n",
    "\n",
    "        train_size = int(len(X) * (1.0 - self.val_ratio))\n",
    "        train_X, valid_X = X[:train_size], X[train_size:]\n",
    "        train_y, valid_y = y[:train_size], y[train_size:]\n",
    "        return train_X, train_y, valid_X, valid_y\n",
    "\n",
    "    def generate_one_epoch(self, data_X, data_y, batch_size):\n",
    "        num_batches = int(len(data_X)) // batch_size\n",
    "        # if batch_size * num_batches < len(self.train_X):\n",
    "        #     num_batches += 1\n",
    "\n",
    "        batch_indices = list(range(num_batches))\n",
    "        random.shuffle(batch_indices)\n",
    "        for j in batch_indices:\n",
    "            batch_X = data_X[j * batch_size: (j + 1) * batch_size]\n",
    "            batch_y = data_y[j * batch_size: (j + 1) * batch_size]\n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordered len: 202\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "boiler_dataset = BoilerDataSet(num_steps=num_steps, val_ratio=valid_ratio)\n",
    "train_X, train_y = boiler_dataset.train_X, boiler_dataset.train_y\n",
    "valid_X, valid_y = boiler_dataset.valid_X, boiler_dataset.valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们的示例中，一共提供了20组数据，设置的时间步长为10.因此，分别有从[0:9]->[10], [1:10]->[11], ... , [9:18]->[19] 共十组（X，y）\\\\\n",
    "其中，我们训练集和测试集的比例为8：2，所以其中训练集有8组，测试集有2组。\\\\\n",
    "train_X (8, 10, 202) 分别为训练集组数、历史步长、数据维度 valid_y(2, 158) 分别为测试集组数和数据维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 8\n",
      "valid samples: 2\n"
     ]
    }
   ],
   "source": [
    "# 打印数据信息\n",
    "print('train samples: {0}'.format(len(train_X)))\n",
    "print('valid samples: {0}'.format(len(valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们的数据中，环境变量有11个，磨煤环节共有89个变量（68个状态和21个动作）、燃烧环节共有81个变量（62个状态和19个动作）、蒸汽循环环节共有21个变量（17个状态和4个动作）\n",
    "\n",
    "统计可知，一共有68+62+17=147个状态，21+19+4=44个动作，加上11个外界环境变量，共202个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=2022):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18516\\1954548937.py:11: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  stacked_outputs = tf.layers.dense(stacked_rnn_outputs, output_size)\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_steps, input_size])\n",
    "y = tf.placeholder(tf.float32, [None, num_steps, output_size])\n",
    "\n",
    "basic_cell = tensorflow.keras.layers.SimpleRNNCell(units=num_neurons)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "# tensorflow.keras.layers.RNN(cell=basic_cell)\n",
    "\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, num_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, output_size)\n",
    "# stacked_outputs = tensorflow.keras.layers.Dense(stacked_rnn_outputs, output_size)\n",
    "\n",
    "outputs = tf.reshape(stacked_outputs, [-1, num_steps, output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SimpleRNN\"\n",
    "logdir = './Simulator/logs/{}-{}-{}-{:.4f}/'.format(\n",
    "    model_name, num_neurons, num_steps, learning_rate)\n",
    "model_dir = logdir + 'saved_models/'\n",
    "\n",
    "from os import path, mkdir\n",
    "# 创建保存结果的文件夹\n",
    "if not path.exists('./Simulator/logs'):\n",
    "    mkdir('./Simulator/logs')\n",
    "if not path.exists(logdir):\n",
    "    mkdir(logdir)\n",
    "if not path.exists(model_dir):\n",
    "    mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "training_optimizer = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tMSE:  0.0585551\n",
      "1 \tMSE:  0.024114087\n",
      "2 \tMSE:  0.012410241\n",
      "3 \tMSE:  0.0056107654\n",
      "4 \tMSE:  0.003611675\n",
      "5 \tMSE:  0.0021131414\n",
      "6 \tMSE:  0.0018291596\n",
      "7 \tMSE:  0.0014946912\n",
      "8 \tMSE:  0.0013507435\n",
      "9 \tMSE:  0.001288366\n",
      "10 \tMSE:  0.00085811195\n",
      "11 \tMSE:  0.0008833653\n",
      "12 \tMSE:  0.00077279325\n",
      "13 \tMSE:  0.00063936325\n",
      "14 \tMSE:  0.0008274237\n",
      "15 \tMSE:  0.00062992657\n",
      "16 \tMSE:  0.0006394714\n",
      "17 \tMSE:  0.00053163804\n",
      "18 \tMSE:  0.00076128886\n",
      "19 \tMSE:  0.0005724366\n",
      "20 \tMSE:  0.000473841\n",
      "21 \tMSE:  0.00066836266\n",
      "22 \tMSE:  0.00053938315\n",
      "23 \tMSE:  0.0005220434\n",
      "24 \tMSE:  0.0004540724\n",
      "25 \tMSE:  0.00043094283\n",
      "26 \tMSE:  0.00045996965\n",
      "27 \tMSE:  0.00057242817\n",
      "28 \tMSE:  0.00045313258\n",
      "29 \tMSE:  0.00054542656\n",
      "30 \tMSE:  0.00057573314\n",
      "31 \tMSE:  0.00060943817\n",
      "32 \tMSE:  0.0004963733\n",
      "33 \tMSE:  0.0006308049\n",
      "34 \tMSE:  0.00039356423\n",
      "35 \tMSE:  0.0003797081\n",
      "36 \tMSE:  0.0004480505\n",
      "37 \tMSE:  0.00045625347\n",
      "38 \tMSE:  0.00045587297\n",
      "39 \tMSE:  0.00033386884\n",
      "40 \tMSE:  0.00041829993\n",
      "41 \tMSE:  0.0003394265\n",
      "42 \tMSE:  0.00037362747\n",
      "43 \tMSE:  0.0004759823\n",
      "44 \tMSE:  0.00037288058\n",
      "45 \tMSE:  0.0003567537\n",
      "46 \tMSE:  0.00034156794\n",
      "47 \tMSE:  0.0003101307\n",
      "48 \tMSE:  0.00030478527\n",
      "49 \tMSE:  0.0004799686\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())     # 初始化全局变量\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter(logdir)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    iteration = 0\n",
    "    for epoch in range(max_epoch):\n",
    "        print('----------epoch {}-----------'.format(epoch))\n",
    "        for batch_X, batch_y in boiler_dataset.generate_one_epoch(train_X, train_y, batch_size):\n",
    "            iteration += 1\n",
    "            sess.run(training_optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "\n",
    "            if iteration % save_log_iter == 0:\n",
    "                summary_writer.add_summary(merged_summ, iter)\n",
    "                if iter % FLAGS.display_iter == 0:\n",
    "                    valid_loss = 0\n",
    "                    for val_batch_X, val_batch_y in boiler_dataset.generate_one_epoch(valid_X, valid_y, FLAGS.batch_size):\n",
    "                        val_data_feed = {\n",
    "                            rnn_model.keep_prob: 1.0,\n",
    "                            rnn_model.inputs: val_batch_X,\n",
    "                            rnn_model.targets: val_batch_y,\n",
    "                        }\n",
    "                        batch_loss = sess.run(rnn_model.loss, val_data_feed)\n",
    "                        valid_loss += batch_loss\n",
    "                    num_batches = int(len(valid_X)) // FLAGS.batch_size\n",
    "                    valid_loss /= num_batches\n",
    "                    valid_losses.append(valid_loss)\n",
    "                    valid_loss_sum = tf.Summary(\n",
    "                        value=[tf.Summary.Value(tag=\"valid_loss\", simple_value=valid_loss)])\n",
    "                    summary_writer.add_summary(valid_loss_sum, iter)\n",
    "\n",
    "                    if valid_loss < min(valid_losses[:-1]):\n",
    "                        print('iter {}\\tvalid_loss = {:.6f}\\tmodel saved!!'.format(\n",
    "                            iter, valid_loss))\n",
    "                        saver.save(sess, model_dir +\n",
    "                                   'model_{}.ckpt'.format(iter))\n",
    "                        saver.save(sess, model_dir + 'final_model.ckpt')\n",
    "                    else:\n",
    "                        print('iter {}\\tvalid_loss = {:.6f}\\t'.format(\n",
    "                            iter, valid_loss))\n",
    "        \n",
    "            mse = loss.eval(feed_dict={X: batch_X, y: batch_y})\n",
    "            print(iteration, \"\\tMSE: \", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
