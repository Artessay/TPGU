{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path, mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tf = tensorflow.compat.v1\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "tf.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "valid_ratio = 0.2\n",
    "\n",
    "input_size = 202\n",
    "num_neurons = 160\n",
    "num_layers = 3\n",
    "output_size = 158\n",
    "\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.95\n",
    "\n",
    "max_epoch = 50\n",
    "batch_size = 1\n",
    "\n",
    "save_log_iter = 10\n",
    "display_iter = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集处理类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoilerDataSet(object):\n",
    "    \"\"\"\n",
    "    first run data_preparation.py to generate data.csv\n",
    "    prepare boiler training and validation dataset\n",
    "    simple version(small action dimension)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_steps, val_ratio=0.1):\n",
    "        self.num_steps = num_steps  # 历史步长\n",
    "        self.val_ratio = val_ratio  # 训练集与测试集比例\n",
    "        \n",
    "        # Read csv file\n",
    "        self.raw_data = pd.read_csv(\"./Simulator/data/sim_train.csv\", index_col='时间戳')\n",
    "\n",
    "        # sort csv file\n",
    "        cols = self.raw_data.columns.tolist()\n",
    "        # print(\"origin len: {0}\".format(len(cols)))\n",
    "        cols = (cols[51:52] + cols[53:59] + cols [60:61] + cols[62:63] + cols[150:152]   # external input \n",
    "            + cols[0:50] + cols[52:53] + cols[122:139]  # Coal Pulverizing state\n",
    "            + cols[50:51] + cols[59:60] + cols[61:62] + cols[63:101] + cols[112:114] + cols[118:122] + cols[139:145] + cols[146:149] + cols[152:158]    # Burning state\n",
    "            + cols[101:112] + cols[114:118] + cols[145:146] + cols[149:150] # Steam Circulation state\n",
    "            + cols[158:173] + cols[196:202] # Coal Pulverizing action\n",
    "            + cols[173:192]                 # Burning action\n",
    "            + cols[192:196])                # Steam Circulation action\n",
    "        print(\"ordered len: {0}\".format(len(cols)))\n",
    "        # self.raw_data = self.raw_data[cols]\n",
    "\n",
    "        # 划分训练集和测试集\n",
    "        self.train_X, self.train_y, self.valid_X, self.valid_y = self.prepare_data(self.raw_data)\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        # split into groups of num_steps\n",
    "\n",
    "        # 取出输入数据，学习num_steps步长的历史，iloc：通过行号获取行数据\n",
    "        X = np.array([data.iloc[i: i + self.num_steps].values\n",
    "                    for i in range(len(data) - self.num_steps)])\n",
    "\n",
    "        # 取出输出数据，预测第num_steps步的值训练，ix / loc 可以通过行号和行标签进行索引\n",
    "        # 这里只要对状态量进行预测即可，0-157列为 'A磨煤机电流':'大渣可燃物含量'\n",
    "        y = np.array([data.iloc[i + self.num_steps, 0:158].values\n",
    "                    for i in range(len(data) - self.num_steps)])\n",
    "\n",
    "        train_size = int(len(X) * (1.0 - self.val_ratio))\n",
    "        train_X, valid_X = X[:train_size], X[train_size:]\n",
    "        train_y, valid_y = y[:train_size], y[train_size:]\n",
    "        return train_X, train_y, valid_X, valid_y\n",
    "\n",
    "    def generate_one_epoch(self, data_X, data_y, batch_size):\n",
    "        num_batches = int(len(data_X)) // batch_size\n",
    "        # if batch_size * num_batches < len(self.train_X):\n",
    "        #     num_batches += 1\n",
    "\n",
    "        batch_indices = list(range(num_batches))\n",
    "        random.shuffle(batch_indices)\n",
    "        for j in batch_indices:\n",
    "            batch_X = data_X[j * batch_size: (j + 1) * batch_size]\n",
    "            batch_y = data_y[j * batch_size: (j + 1) * batch_size]\n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordered len: 202\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "boiler_dataset = BoilerDataSet(num_steps=num_steps, val_ratio=valid_ratio)\n",
    "train_X, train_y = boiler_dataset.train_X, boiler_dataset.train_y\n",
    "valid_X, valid_y = boiler_dataset.valid_X, boiler_dataset.valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们的示例中，一共提供了20组数据，设置的时间步长为10.因此，分别有从[0:9]->[10], [1:10]->[11], ... , [9:18]->[19] 共十组（X，y）\\\\\n",
    "其中，我们训练集和测试集的比例为8：2，所以其中训练集有8组，测试集有2组。\\\\\n",
    "train_X (8, 10, 202) 分别为训练集组数、历史步长、数据维度 valid_y(2, 158) 分别为测试集组数和数据维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 8\n",
      "valid samples: 2\n"
     ]
    }
   ],
   "source": [
    "# 打印数据信息\n",
    "print('train samples: {0}'.format(len(train_X)))\n",
    "print('valid samples: {0}'.format(len(valid_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们的数据中，环境变量有11个，磨煤环节共有89个变量（68个状态和21个动作）、燃烧环节共有81个变量（62个状态和19个动作）、蒸汽循环环节共有21个变量（17个状态和4个动作）\n",
    "\n",
    "统计可知，一共有68+62+17=147个状态，21+19+4=44个动作，加上11个外界环境变量，共202个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=2022):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11328\\2356377417.py:20: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  outputs = tf.layers.dense(rnn_outputs, output_size)\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow import keras\n",
    "# def RNNSimulatorModel():\n",
    "#     model = keras.Sequential()\n",
    "#     return model\n",
    "# model = RNNSimulatorModel()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_steps, input_size])\n",
    "y = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# basic_cell = tensorflow.keras.layers.LSTM(units=num_neurons, activation='tanh', return_sequences=True)\n",
    "# lstm_cells = [tf.nn.rnn_cell.BasicLSTMCell(num_units=num_neurons)\n",
    "#               for layer in range(num_layers)]\n",
    "# multi_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
    "# rnn_outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "rnn_layer = tensorflow.keras.layers.RNN(\n",
    "            [tensorflow.keras.layers.LSTMCell(128),\n",
    "             tensorflow.keras.layers.LSTMCell(256)])\n",
    "\n",
    "rnn_outputs = rnn_layer(X)\n",
    "outputs = tf.layers.dense(rnn_outputs, output_size)\n",
    "# stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, num_neurons])\n",
    "# stacked_outputs = tf.layers.dense(stacked_rnn_outputs, output_size)\n",
    "# # stacked_outputs = tensorflow.keras.layers.Dense(stacked_rnn_outputs, output_size)\n",
    "\n",
    "# outputs = tf.reshape(stacked_outputs, [-1, num_steps, output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(outputs - y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "training_optimizer = optimizer.minimize(loss)\n",
    "\n",
    "loss_summary = tf.summary.scalar(\"loss_mse_train\", loss)\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LSTM-keras\"\n",
    "logdir = './Simulator/logs/{}-{}-{:.4f}/'.format(\n",
    "    model_name,  num_steps, learning_rate)\n",
    "model_dir = logdir + 'saved_models/'\n",
    "\n",
    "# 创建保存结果的文件夹\n",
    "if not path.exists('./Simulator/logs'):\n",
    "    mkdir('./Simulator/logs')\n",
    "if not path.exists(logdir):\n",
    "    mkdir(logdir)\n",
    "if not path.exists(model_dir):\n",
    "    mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------epoch 0-----------\n",
      "epoch:  0 \tMSE:  0.009102102\n",
      "----------epoch 1-----------\n",
      "epoch:  1 \tMSE:  0.005789786\n",
      "----------epoch 2-----------\n",
      "iter 20\tvalid_loss = 0.004482\tmodel saved\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_20.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_20.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_20.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "epoch:  2 \tMSE:  0.0018843717\n",
      "----------epoch 3-----------\n",
      "epoch:  3 \tMSE:  0.0014699928\n",
      "----------epoch 4-----------\n",
      "iter 40\tvalid_loss = 0.002296\tmodel saved\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_40.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_40.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_40.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "epoch:  4 \tMSE:  0.0008334755\n",
      "----------epoch 5-----------\n",
      "epoch:  5 \tMSE:  0.00056982675\n",
      "----------epoch 6-----------\n",
      "epoch:  6 \tMSE:  0.00044753676\n",
      "----------epoch 7-----------\n",
      "iter 60\tvalid_loss = 0.002060\tmodel saved\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_60.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_60.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_60.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "epoch:  7 \tMSE:  0.00018447192\n",
      "----------epoch 8-----------\n",
      "epoch:  8 \tMSE:  0.00017419433\n",
      "----------epoch 9-----------\n",
      "iter 80\tvalid_loss = 0.001856\tmodel saved\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_80.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_80.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_80.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "epoch:  9 \tMSE:  0.0007993897\n",
      "----------epoch 10-----------\n",
      "epoch:  10 \tMSE:  0.00034729135\n",
      "----------epoch 11-----------\n",
      "epoch:  11 \tMSE:  0.0003817466\n",
      "----------epoch 12-----------\n",
      "iter 100\tvalid_loss = 0.002027\t\n",
      "epoch:  12 \tMSE:  9.395516e-05\n",
      "----------epoch 13-----------\n",
      "epoch:  13 \tMSE:  0.00041238486\n",
      "----------epoch 14-----------\n",
      "iter 120\tvalid_loss = 0.002046\t\n",
      "epoch:  14 \tMSE:  0.00045676925\n",
      "----------epoch 15-----------\n",
      "epoch:  15 \tMSE:  0.0005765454\n",
      "----------epoch 16-----------\n",
      "epoch:  16 \tMSE:  0.0006656328\n",
      "----------epoch 17-----------\n",
      "iter 140\tvalid_loss = 0.001680\tmodel saved\n",
      "WARNING:tensorflow:From d:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1066: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_140.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_140.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\model_140.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.data-00000-of-00001\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.index\n",
      "INFO:tensorflow:14000\n",
      "INFO:tensorflow:./Simulator/logs/LSTM-keras-10-0.0010/saved_models\\final_model.ckpt.meta\n",
      "INFO:tensorflow:14600\n",
      "epoch:  17 \tMSE:  0.00056579796\n",
      "----------epoch 18-----------\n",
      "epoch:  18 \tMSE:  0.00031201242\n",
      "----------epoch 19-----------\n",
      "iter 160\tvalid_loss = 0.001808\t\n",
      "epoch:  19 \tMSE:  0.00025371555\n",
      "----------epoch 20-----------\n",
      "epoch:  20 \tMSE:  0.0004538889\n",
      "----------epoch 21-----------\n",
      "epoch:  21 \tMSE:  0.0003431179\n",
      "----------epoch 22-----------\n",
      "iter 180\tvalid_loss = 0.002004\t\n",
      "epoch:  22 \tMSE:  0.00034542326\n",
      "----------epoch 23-----------\n",
      "epoch:  23 \tMSE:  0.0004508634\n",
      "----------epoch 24-----------\n",
      "iter 200\tvalid_loss = 0.002111\t\n",
      "epoch:  24 \tMSE:  0.00018413324\n",
      "----------epoch 25-----------\n",
      "epoch:  25 \tMSE:  0.00036173884\n",
      "----------epoch 26-----------\n",
      "epoch:  26 \tMSE:  0.0008129563\n",
      "----------epoch 27-----------\n",
      "iter 220\tvalid_loss = 0.001903\t\n",
      "epoch:  27 \tMSE:  0.00015606175\n",
      "----------epoch 28-----------\n",
      "epoch:  28 \tMSE:  0.00017976637\n",
      "----------epoch 29-----------\n",
      "iter 240\tvalid_loss = 0.001845\t\n",
      "epoch:  29 \tMSE:  0.00020308472\n",
      "----------epoch 30-----------\n",
      "epoch:  30 \tMSE:  0.00023236343\n",
      "----------epoch 31-----------\n",
      "epoch:  31 \tMSE:  0.00082362245\n",
      "----------epoch 32-----------\n",
      "iter 260\tvalid_loss = 0.001950\t\n",
      "epoch:  32 \tMSE:  0.00044353123\n",
      "----------epoch 33-----------\n",
      "epoch:  33 \tMSE:  0.0006695768\n",
      "----------epoch 34-----------\n",
      "iter 280\tvalid_loss = 0.002004\t\n",
      "epoch:  34 \tMSE:  0.00036419753\n",
      "----------epoch 35-----------\n",
      "epoch:  35 \tMSE:  0.0008514338\n",
      "----------epoch 36-----------\n",
      "epoch:  36 \tMSE:  0.0004246664\n",
      "----------epoch 37-----------\n",
      "iter 300\tvalid_loss = 0.001853\t\n",
      "epoch:  37 \tMSE:  0.00028368298\n",
      "----------epoch 38-----------\n",
      "epoch:  38 \tMSE:  0.0003293453\n",
      "----------epoch 39-----------\n",
      "iter 320\tvalid_loss = 0.002218\t\n",
      "epoch:  39 \tMSE:  0.00023401689\n",
      "----------epoch 40-----------\n",
      "epoch:  40 \tMSE:  0.00080391427\n",
      "----------epoch 41-----------\n",
      "epoch:  41 \tMSE:  0.00063760276\n",
      "----------epoch 42-----------\n",
      "iter 340\tvalid_loss = 0.002068\t\n",
      "epoch:  42 \tMSE:  0.00025981333\n",
      "----------epoch 43-----------\n",
      "epoch:  43 \tMSE:  0.00021706286\n",
      "----------epoch 44-----------\n",
      "iter 360\tvalid_loss = 0.002017\t\n",
      "epoch:  44 \tMSE:  0.00072500063\n",
      "----------epoch 45-----------\n",
      "epoch:  45 \tMSE:  0.0003233087\n",
      "----------epoch 46-----------\n",
      "epoch:  46 \tMSE:  0.0005181619\n",
      "----------epoch 47-----------\n",
      "iter 380\tvalid_loss = 0.002238\t\n",
      "epoch:  47 \tMSE:  0.00034628727\n",
      "----------epoch 48-----------\n",
      "epoch:  48 \tMSE:  0.0005420177\n",
      "----------epoch 49-----------\n",
      "iter 400\tvalid_loss = 0.001906\t\n",
      "epoch:  49 \tMSE:  0.00041239779\n"
     ]
    }
   ],
   "source": [
    "summary_writer = tf.summary.FileWriter(logdir)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())     # 初始化全局变量\n",
    "    \n",
    "    iteration = 0           # 迭代数\n",
    "    valid_losses = [np.inf] # 损失值集合\n",
    "\n",
    "    for epoch in range(max_epoch):\n",
    "        print('----------epoch {}-----------'.format(epoch))\n",
    "        \n",
    "        for batch_X, batch_y in boiler_dataset.generate_one_epoch(train_X, train_y, batch_size):\n",
    "            iteration += 1\n",
    "            sess.run(training_optimizer, feed_dict={X: batch_X, y: batch_y})\n",
    "            \n",
    "            summary = sess.run(merged_summary, feed_dict={X: batch_X, y: batch_y})\n",
    "\n",
    "            if iteration % save_log_iter == 0:\n",
    "                summary_writer.add_summary(summary, iteration)\n",
    "            \n",
    "            if iteration % display_iter == 0:\n",
    "                valid_loss = 0\n",
    "                for valid_batch_X, valid_batch_y in boiler_dataset.generate_one_epoch(valid_X, valid_y, batch_size):\n",
    "                    batch_mse = loss.eval(feed_dict={X: valid_batch_X, y: valid_batch_y})\n",
    "                    batch_loss = batch_mse      # @todo l2 loss\n",
    "                    valid_loss += batch_loss\n",
    "                num_batches = int(len(valid_X)) // batch_size\n",
    "                valid_loss /= num_batches       # 平均每个批次的损失\n",
    "                valid_losses.append(valid_loss)\n",
    "                valid_loss_sum = tf.Summary(\n",
    "                    value=[tf.Summary.Value(tag=\"valid_loss\", simple_value=valid_loss)])\n",
    "                summary_writer.add_summary(valid_loss_sum, iteration)\n",
    "\n",
    "                if valid_loss < min(valid_losses[:-1]):\n",
    "                    print('iter {}\\tvalid_loss = {:.6f}\\tmodel saved'.format(\n",
    "                        iteration, valid_loss))\n",
    "                    saver.save(sess, model_dir +\n",
    "                                'model_{}.ckpt'.format(iteration))\n",
    "                    saver.save(sess, model_dir + 'final_model.ckpt')\n",
    "                else:\n",
    "                    print('iter {}\\tvalid_loss = {:.6f}\\t'.format(\n",
    "                        iteration, valid_loss))\n",
    "\n",
    "        mse = loss.eval(feed_dict={X: batch_X, y: batch_y})\n",
    "        print(\"epoch: \", epoch, \"\\tMSE: \", mse)\n",
    "\n",
    "summary_writer.flush()\n",
    "summary_writer.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary.py的函数\n",
    "\n",
    "1、tf.summary.scalar\n",
    "\n",
    "用于保存标量或单个数值，一般用来保存loss，accuary，学习率等数据，比较常用\n",
    "\n",
    "~~~python\n",
    "tf.summary.scalar(name,tensor,collections=None,family=None)\n",
    "~~~\n",
    "\n",
    "name：要保存的数据的命名。在TensorBoard中也用作系列名称。\n",
    "\n",
    "tensor：要可视化的数据，张量。在scalar函数中应该是一个标量，如当前的学习率、Loss等\n",
    "\n",
    "collections：定义保存的数据归于哪个集合。默认为[GraphKeys.SUMMARIES]\n",
    "\n",
    "family：如果定义，在Tensorboard显示的时候，将作为前缀加在变量名前\n",
    "\n",
    " \n",
    "\n",
    "2、tf.summary.histogram\n",
    "\n",
    "输出带直方图的汇总的protocol buffer数据，一般用来显示训练过程中变量的分布情况\n",
    "\n",
    "~~~python\n",
    "tf.summary.histogram(name,values,collections=None,family=None)\n",
    "~~~\n",
    "\n",
    "values：要可视化的数据，可以是任意形状和大小的张量数据\n",
    "\n",
    "其它三个参数跟上面一样。\n",
    "\n",
    " \n",
    "\n",
    "3、tf.summary.image\n",
    "\n",
    "输出带图像的protocol buffer数据，汇总数据的图像的的形式如下： ’ tag /image/0’, ’ tag /image/1’…，如：input/image/0等。\n",
    "\n",
    "~~~python\n",
    "tf.summary.image(name,tensor,max_outputs=3,\n",
    "    collections=None,\n",
    "    family=None)\n",
    "~~~\n",
    "\n",
    "tensor：形状为[批量数、高度、宽度、通道数]的4阶张量，类型为 uint8 或 float32 ，其中通道数为1、3或4。\n",
    "\n",
    "max_outputs：生成图像的批处理元素的最大数目。\n",
    "\n",
    "其它同上。\n",
    "\n",
    " \n",
    "\n",
    "4、tf.summary.audio\n",
    "\n",
    "输出带音频的protocol buffer数据。音频是由张量构建的，张量必须是三维的[批量大小，帧，通道数]或二维的[批量大小，帧]。\n",
    "\n",
    "~~~python\n",
    "tf.summary.audio(name,tensor,sample_rate,\n",
    "    max_outputs=3,\n",
    "    collections=None,\n",
    "    family=None)\n",
    "~~~\n",
    "\n",
    "tensor：形状为[批量大小，帧，通道数]的三阶张量或形状[批量大小，帧]的二阶张量。类型为float32\n",
    "\n",
    "sample_rate：以赫兹表示的信号采样率的标量float32张量。\n",
    "\n",
    "max_outputs：为其生成音频的批处理元素的最大数目\n",
    "\n",
    "其它同上\n",
    "\n",
    " \n",
    "\n",
    "5、tf.summary.merge\n",
    "\n",
    "对指定的多个值进行联合输出。运行op时，如果要合并的摘要中的多个值使用同一个标记，则会报告InvalidArgument错误。\n",
    "\n",
    "~~~python\n",
    "tf.summary.merge(inputs,collections=None,name=None)\n",
    "inputs：包含protocol buffers数据的字符串张量列表\n",
    "~~~\n",
    "\n",
    "collections：定义保存的数据归于哪个集合。默认为[]\n",
    "\n",
    "name：操作的名称\n",
    "\n",
    " \n",
    "\n",
    "6、tf.summary.merge_all\n",
    "\n",
    "对所有值进行联合输出。可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了\n",
    "\n",
    "~~~python\n",
    "tf.summary.merge_all(key=tf.GraphKeys.SUMMARIES,scope=None,name=None)\n",
    "~~~\n",
    "\n",
    "key：定义保存的数据归于哪个集合。默认为[GraphKeys.SUMMARIES]\n",
    "\n",
    "scope：用于筛选摘要操作的可选范围，使用re.match\n",
    "\n",
    "name：操作的名称"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
