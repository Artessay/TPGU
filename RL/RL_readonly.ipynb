{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tf = tensorflow.compat.v1\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "ACTOR_LAYER1_SIZE = 256\n",
    "ACTOR_LAYER2_SIZE = 256\n",
    "ACTOR_LEARNING_RATE = 0.0001\n",
    "ACTOR_TAU = 0.001\n",
    "\n",
    "CRITIC_LAYER1_SIZE = 256\n",
    "CRITIC_LAYER2_SIZE = 256\n",
    "CRITIC_LEARNING_RATE = 0.0001\n",
    "CRITIC_TAU = 0.001\n",
    "CRITIC_L2 = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial, name)\n",
    "\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.03, shape=shape)\n",
    "    return tf.Variable(initial, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def load_network(self):\\n        self.saver = tf.train.Saver()\\n        checkpoint = tf.train.get_checkpoint_state(\"saved_actor_networks\")\\n        if checkpoint and checkpoint.model_checkpoint_path:\\n            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\\n            print \"Successfully loaded:\", checkpoint.model_checkpoint_path\\n        else:\\n            print \"Could not find old network weights\"\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActorNetwork(object):\n",
    "    \"\"\" Map: state + limit_load -> action \"\"\"\n",
    "\n",
    "    def __init__(self, sess, input_config, load_model, summ_writer):\n",
    "        self.sess = sess\n",
    "        self.state_dimension = input_config.state_dimension\n",
    "        self.action_dimension = input_config.action_dimension\n",
    "        self.save_iter = input_config.save_iter  # interval of saving log\n",
    "        self.save_path = input_config.model_save_path + \"/actor\"  # interval of saving model\n",
    "        self.log_iter = input_config.log_iter  # logging interval in training phase\n",
    "        self.log_path = input_config.log_path  # log path\n",
    "        self.clip_norm = input_config.clip_norm\n",
    "        self.step = 0\n",
    "\n",
    "        self.train_writer = summ_writer\n",
    "\n",
    "        # create actor network\n",
    "        self.state_input, self.action_output, self.net = self.create_network(self.state_dimension, self.action_dimension)\n",
    "        # create target actor network\n",
    "        self.target_state_input, self.target_action_output, self.target_update, self.target_net = self.create_target_network(\n",
    "            self.state_dimension, self.action_dimension, self.net)\n",
    "        self.create_training_method()\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        # self.saver = tf.train.Saver(tf.global_variables(scope=scope))\n",
    "        if load_model:\n",
    "            # restore actor network\n",
    "            print('actor network restore weights')\n",
    "            self.saver.restore(sess=self.sess, save_path=tf.train.latest_checkpoint(input_config.load_path))\n",
    "        else:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.q_gradient_input = tf.placeholder(\"float\", [None, self.action_dimension])\n",
    "        self.unnormalized_actor_gradients = tf.gradients(self.action_output, self.net, -self.q_gradient_input)\n",
    "        # self.actor_gradients = list(map(lambda x: tf.div(x, BATCH_SIZE), self.unnormalized_actor_gradients))\n",
    "        # gradients clip\n",
    "        # self.actor_gradients, _ = tf.clip_by_global_norm(self.actor_gradients, clip_norm=self.clip_norm)\n",
    "\n",
    "        # extra_ops = tf.get_collection('actor_parameters_extra_option')\n",
    "        # apply_op = tf.train.AdamOptimizer(ACTOR_LEARNING_RATE).apply_gradients(zip(self.unnormalized_actor_gradients, self.net))\n",
    "        apply_op = tf.train.RMSPropOptimizer(ACTOR_LEARNING_RATE).apply_gradients(zip(self.unnormalized_actor_gradients, self.net))\n",
    "\n",
    "        # train_ops = [apply_op] + extra_ops\n",
    "        # self.optimizer = tf.group(*apply_op)\n",
    "        self.optimizer = apply_op\n",
    "\n",
    "        diff = self.action_output - self.target_action_output\n",
    "        self.mse = tf.reduce_mean(tf.square(diff))\n",
    "        pretrain_grad = tf.gradients(self.mse, self.net)\n",
    "        self.pretrain_update = tf.train.AdamOptimizer(ACTOR_LEARNING_RATE).apply_gradients(\n",
    "            zip(pretrain_grad, self.net))\n",
    "\n",
    "\n",
    "\n",
    "    def create_network(self, state_dimension, action_dimension):\n",
    "        ACTOR_LAYER1_SIZE = ACTOR_LAYER1_SIZE\n",
    "        ACTOR_LAYER2_SIZE = ACTOR_LAYER2_SIZE\n",
    "\n",
    "        state_input = tf.placeholder(\"float\", [None, state_dimension])\n",
    "\n",
    "        w1 = self.variable([state_dimension, ACTOR_LAYER1_SIZE], state_dimension)\n",
    "        b1 = self.variable([ACTOR_LAYER1_SIZE], state_dimension)\n",
    "        w2 = self.variable([ACTOR_LAYER1_SIZE, ACTOR_LAYER2_SIZE], ACTOR_LAYER1_SIZE)\n",
    "        b2 = self.variable([ACTOR_LAYER2_SIZE], ACTOR_LAYER1_SIZE)\n",
    "        w3 = tf.Variable(tf.random_uniform([ACTOR_LAYER2_SIZE, action_dimension], -3e-3, 3e-3))\n",
    "        b3 = tf.Variable(tf.random_uniform([action_dimension], -3e-3, 3e-3))\n",
    "\n",
    "        layer1 = tf.nn.relu(tf.matmul(state_input, w1) + b1)\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "        action_output = tf.sigmoid(tf.matmul(layer2, w3) + b3)\n",
    "        out_summ = tf.summary.histogram('action_output', action_output)\n",
    "\n",
    "        w1_summ = tf.summary.histogram('W1', values=w1)\n",
    "        b1_summ = tf.summary.histogram('b1', values=b1)\n",
    "\n",
    "        w2_summ = tf.summary.histogram('W2', values=w2)\n",
    "        b2_summ = tf.summary.histogram('b2', values=b2)\n",
    "\n",
    "        w3_summ = tf.summary.histogram('W3', values=w3)\n",
    "        b3_summ = tf.summary.histogram('b3', values=b3)\n",
    "\n",
    "        self.merged_summ = tf.summary.merge([out_summ, w1_summ, b1_summ, w2_summ, b2_summ, w3_summ, b3_summ])\n",
    "        # self.merged_summ = tf.summary.merge([out_summ])\n",
    "\n",
    "        return state_input, action_output, [w1, b1, w2, b2, w3, b3]\n",
    "\n",
    "    def create_target_network(self, state_dimension, action_dimension, net):\n",
    "        state_input = tf.placeholder(\"float\", [None, state_dimension])\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=1 - ACTOR_TAU)\n",
    "        target_update = ema.apply(net)\n",
    "        target_net = [ema.average(x) for x in net]\n",
    "\n",
    "        layer1 = tf.nn.relu(tf.matmul(state_input, target_net[0]) + target_net[1])\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, target_net[2]) + target_net[3])\n",
    "\n",
    "        action_output = tf.tanh(tf.matmul(layer2, target_net[4]) + target_net[5])\n",
    "\n",
    "        return state_input, action_output, target_update, target_net\n",
    "\n",
    "    def update_target(self):\n",
    "        self.sess.run(self.target_update)\n",
    "\n",
    "    def train(self, q_gradient_batch, state_batch):\n",
    "        train_feed_dict = {\n",
    "            self.q_gradient_input: q_gradient_batch,\n",
    "            self.state_input: state_batch\n",
    "        }\n",
    "        summ, _ = self.sess.run([self.merged_summ, self.optimizer], feed_dict=train_feed_dict)\n",
    "        # _ = self.sess.run([self.optimizer], feed_dict=train_feed_dict)\n",
    "\n",
    "        # save actor network\n",
    "        if self.step % self.save_iter == 0:\n",
    "            self.saver.save(self.sess, save_path=self.save_path, global_step=self.step)\n",
    "\n",
    "        if self.step % self.log_iter == 0:\n",
    "            self.train_writer.add_summary(summ, global_step=self.step)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "    def pretrain(self, state, label):\n",
    "        # cost\n",
    "        train_feed_dict = {self.state_input: state, self.target_action_output: label}\n",
    "        _, net, mse = self.sess.run([self.pretrain_update, self.net, self.mse], feed_dict=train_feed_dict)\n",
    "        # save actor network\n",
    "        if self.step % self.save_iter == 0:\n",
    "            self.saver.save(self.sess, save_path=self.save_path, global_step=self.step)\n",
    "\n",
    "        self.step += 1\n",
    "        return net, mse\n",
    "\n",
    "    def actions(self, state_batch):\n",
    "        return self.sess.run(self.action_output, feed_dict={\n",
    "            self.state_input: state_batch\n",
    "        })\n",
    "\n",
    "    def action(self, state):\n",
    "        return self.sess.run(self.action_output, feed_dict={\n",
    "            self.state_input: [state]\n",
    "        })[0]\n",
    "\n",
    "    def target_actions(self, state_batch):\n",
    "        return self.sess.run(self.target_action_output, feed_dict={\n",
    "            self.target_state_input: state_batch\n",
    "        })\n",
    "\n",
    "        # f fan-in size\n",
    "    def variable(self, shape, f):\n",
    "        return tf.Variable(tf.random_uniform(shape, -1 / math.sqrt(f), 1 / math.sqrt(f)))\n",
    "\n",
    "    def save_network(self, episode):\n",
    "        print('save actor-network...', episode)\n",
    "        self.saver.save(self.sess, 'saved_actor_networks/' + 'actor-network', global_step=episode)\n",
    "\n",
    "'''\n",
    "    def load_network(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(\"saved_actor_networks\")\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print \"Successfully loaded:\", checkpoint.model_checkpoint_path\n",
    "        else:\n",
    "            print \"Could not find old network weights\"\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def load_network(self):\\n        self.saver = tf.train.Saver()\\n        checkpoint = tf.train.get_checkpoint_state(\"saved_cost_critic_networks\")\\n        if checkpoint and checkpoint.model_checkpoint_path:\\n            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\\n            print \"Successfully loaded:\", checkpoint.model_checkpoint_path\\n        else:\\n            print \"Could not find old network weights\"\\n    def save_network(self,time_step):\\n        print \\'save cost-critic-network...\\',time_step\\n        self.saver.save(self.sess, \\'saved_cost_critic_networks/\\' + \\'cost-critic-network\\', global_step = time_step)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CostCriticNetwork(object):\n",
    "    def __init__(self, sess, input_config, summ_writer):\n",
    "        self.time_step = 0\n",
    "        self.sess = sess\n",
    "        self.state_dimension = input_config.state_dimension\n",
    "        self.action_dimension = input_config.action_dimension\n",
    "        self.clip_norm = input_config.clip_norm\n",
    "        self.step = 0\n",
    "        self.log_iter = input_config.log_iter  # logging interval in training phase\n",
    "        self.log_path = input_config.log_path  # logging interval in training phase\n",
    "\n",
    "        self.train_writer_cost = summ_writer\n",
    "\n",
    "\n",
    "        # create cost network\n",
    "        self.state_input, \\\n",
    "        self.action_input, \\\n",
    "        self.cost_value_output, \\\n",
    "        self.cost_net = self.create_cost_network(self.state_dimension, self.action_dimension)\n",
    "\n",
    "        # create target cost network (the same structure with cost network)\n",
    "        self.target_state_input, \\\n",
    "        self.target_action_input, \\\n",
    "        self.target_cost_value_output, \\\n",
    "        self.cost_target_update = self.create_target_cost_network(self.state_dimension, self.action_dimension, self.cost_net)\n",
    "\n",
    "        self.create_training_method()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "\n",
    "    def create_training_method(self):\n",
    "        # Define training optimizer\n",
    "        self.z_input = tf.placeholder(\"float\", [None, 1])\n",
    "        weight_decay = tf.add_n([CRITIC_L2 * tf.nn.CRITIC_L2_loss(var) for var in self.cost_net])\n",
    "        self.cost_cost = tf.reduce_mean(tf.square(self.z_input - self.cost_value_output)) + weight_decay\n",
    "        self.optimizer = tf.train.AdamOptimizer(CRITIC_LEARNING_RATE).minimize(self.cost_cost)\n",
    "        self.action_gradients_cost = tf.gradients(self.cost_value_output, self.action_input)\n",
    "\n",
    "    def create_cost_network(self, state_dimension, action_dimension):\n",
    "        # the layer size could be changed\n",
    "        CRITIC_LAYER1_SIZE = CRITIC_LAYER1_SIZE\n",
    "        CRITIC_LAYER2_SIZE = CRITIC_LAYER2_SIZE\n",
    "\n",
    "        state_input = tf.placeholder(\"float\", [None, state_dimension])\n",
    "        action_input = tf.placeholder(\"float\", [None, action_dimension])\n",
    "\n",
    "        # Input -> Hidden Layer\n",
    "        w1 = weight_variable([state_dimension, CRITIC_LAYER1_SIZE])\n",
    "        b1 = bias_variable([CRITIC_LAYER1_SIZE])\n",
    "        # Hidden Layer -> Hidden Layer + Action\n",
    "        w2 = weight_variable([CRITIC_LAYER1_SIZE, CRITIC_LAYER2_SIZE])\n",
    "        w2a = weight_variable([action_dimension, CRITIC_LAYER2_SIZE])\n",
    "        b2 = bias_variable([CRITIC_LAYER2_SIZE])\n",
    "        # Hidden Layer -> Output (Q)\n",
    "        w3 = weight_variable([CRITIC_LAYER2_SIZE, 1])\n",
    "        b3 = bias_variable([1])\n",
    "\n",
    "        # 1st Hidden layer, OPTION: Softmax, relu, tanh or sigmoid\n",
    "        h1 = tf.nn.relu(tf.matmul(state_input, w1) + b1)\n",
    "        # 2nd Hidden layer, OPTION: Softmax, relu, tanh or sigmoid\n",
    "        # Action inserted here\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, w2) + tf.matmul(action_input, w2a) + b2)\n",
    "\n",
    "        cost_value_output = tf.matmul(h2, w3) + b3\n",
    "\n",
    "        return state_input, action_input, cost_value_output, [w1, b1, w2, w2a, b2, w3, b3]\n",
    "\n",
    "    def create_target_cost_network(self, state_dimension, action_dimension, net):\n",
    "        state_input = tf.placeholder(\"float\", [None, state_dimension])\n",
    "        action_input = tf.placeholder(\"float\", [None, action_dimension])\n",
    "\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=1 - CRITIC_TAU)\n",
    "        target_update = ema.apply(net)\n",
    "        target_net = [ema.average(x) for x in net]\n",
    "\n",
    "        layer1 = tf.nn.relu(tf.matmul(state_input, target_net[0]) + target_net[1])\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, target_net[2]) + tf.matmul(action_input, target_net[3]) + target_net[4])\n",
    "        cost_value_output = tf.identity(tf.matmul(layer2, target_net[5]) + target_net[6])\n",
    "\n",
    "        return state_input, action_input, cost_value_output, target_update\n",
    "\n",
    "    def update_target(self):\n",
    "        self.sess.run(self.cost_target_update)\n",
    "\n",
    "    def train(self, z_batch, state_batch, action_batch):\n",
    "        # c_loss_summ = tf.summary.scalar('cost_critic_loss', self.cost_cost)\n",
    "        # self.merged_cost = tf.summary.merge([c_loss_summ])\n",
    "\n",
    "        train_feed_dict = {\n",
    "            self.z_input: z_batch,\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch\n",
    "        }\n",
    "        _, cost_critic_loss, cost_action_grad_norm = \\\n",
    "            self.sess.run([self.optimizer, self.cost_cost, self.action_gradients_cost], train_feed_dict)\n",
    "\n",
    "        # if self.step % self.log_iter == 0:\n",
    "        #     self.train_writer_cost.add_summary(merged_summ_cost, global_step=self.step)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        return cost_critic_loss, cost_action_grad_norm\n",
    "\n",
    "    def pretrain(self, z_batch, state_batch, action_batch):\n",
    "        train_feed_dict = {\n",
    "            self.z_input: z_batch,\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch\n",
    "        }\n",
    "        _, cost_critic_loss = self.sess.run([self.optimizer, self.cost_cost], train_feed_dict)\n",
    "        return cost_critic_loss\n",
    "\n",
    "    def gradients(self, state_batch, action_batch):\n",
    "        return self.sess.run(self.action_gradients_cost, feed_dict={\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch\n",
    "        })[0]\n",
    "\n",
    "    def target_cost(self, state_batch, action_batch):\n",
    "        return self.sess.run(self.target_cost_value_output, feed_dict={\n",
    "            self.target_state_input: state_batch,\n",
    "            self.target_action_input: action_batch\n",
    "        })\n",
    "\n",
    "    def cost_value(self, state_batch, action_batch):\n",
    "        return self.sess.run(self.cost_value_output, feed_dict={\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch})\n",
    "\n",
    "        # f fan-in size\n",
    "    def variable(self, shape, f):\n",
    "        return tf.Variable(tf.random_uniform(shape, -1 / math.sqrt(f), 1 / math.sqrt(f)))\n",
    "\n",
    "'''\n",
    "    def load_network(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(\"saved_cost_critic_networks\")\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print \"Successfully loaded:\", checkpoint.model_checkpoint_path\n",
    "        else:\n",
    "            print \"Could not find old network weights\"\n",
    "    def save_network(self,time_step):\n",
    "        print 'save cost-critic-network...',time_step\n",
    "        self.saver.save(self.sess, 'saved_cost_critic_networks/' + 'cost-critic-network', global_step = time_step)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def load_network(self):\\n        self.saver = tf.train.Saver()\\n        checkpoint = tf.train.get_checkpoint_state(\"saved_reward_critic_networks\")\\n        if checkpoint and checkpoint.model_checkpoint_path:\\n            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\\n            print \"Successfully loaded:\", checkpoint.model_checkpoint_path\\n        else:\\n            print \"Could not find old network weights\"\\n    def save_network(self,time_step):\\n        print \\'save reward-critic-network...\\',time_step\\n        self.saver.save(self.sess, \\'saved_reward_critic_networks/\\' + \\'reward-critic-network\\', global_step = time_step)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RewardCriticNetwork(object):\n",
    "    def __init__(self, sess, input_config, summ_writer):\n",
    "        self.time_step = 0\n",
    "        self.sess = sess\n",
    "        self.state_dimension = input_config.state_dimension\n",
    "        self.action_dimension = input_config.action_dimension\n",
    "        self.clip_norm = input_config.clip_norm\n",
    "        self.step = 0\n",
    "        self.log_iter = input_config.log_iter  # logging interval in training phase\n",
    "        self.log_path = input_config.log_path  # logging interval in training phase\n",
    "\n",
    "        self.train_writer = summ_writer\n",
    "\n",
    "        # create reward network\n",
    "        self.state_input, \\\n",
    "        self.action_input, \\\n",
    "        self.reward_value_output, \\\n",
    "        self.net = self.create_reward_network(self.state_dimension, self.action_dimension)\n",
    "\n",
    "        # create target reward network (the same structure with reward network)\n",
    "        self.target_state_input, \\\n",
    "        self.target_action_input, \\\n",
    "        self.target_reward_value_output, \\\n",
    "        self.target_update = self.create_target_reward_network(self.state_dimension, self.action_dimension, self.net)\n",
    "\n",
    "        self.create_training_method()\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "    def create_training_method(self):\n",
    "        # Define training optimizer\n",
    "        self.y_input = tf.placeholder(\"float\", [None, 1])\n",
    "        weight_decay = tf.add_n([CRITIC_L2 * tf.nn.l2_loss(var) for var in self.net])\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - self.reward_value_output)) + weight_decay\n",
    "        self.optimizer = tf.train.AdamOptimizer(CRITIC_LEARNING_RATE).minimize(self.cost)\n",
    "        self.action_gradients = tf.gradients(self.reward_value_output, self.action_input)\n",
    "\n",
    "\n",
    "    def create_reward_network(self, state_dimension, action_dimension):\n",
    "        # the layer size could be changed\n",
    "        layer1_size = CRITIC_LAYER1_SIZE\n",
    "        layer2_size = CRITIC_LAYER2_SIZE\n",
    "\n",
    "        state_input = tf.placeholder(\"float\", [None, state_dimension])\n",
    "        action_input = tf.placeholder(\"float\", [None, action_dimension])\n",
    "\n",
    "        # Input -> Hidden Layer\n",
    "        w1 = weight_variable([state_dimension, layer1_size])\n",
    "        b1 = bias_variable([layer1_size])\n",
    "        # Hidden Layer -> Hidden Layer + Action\n",
    "        w2 = weight_variable([layer1_size, layer2_size])\n",
    "        w2a = weight_variable([action_dimension, layer2_size])\n",
    "        b2 = bias_variable([layer2_size])\n",
    "        # Hidden Layer -> Output (Q)\n",
    "        w3 = weight_variable([layer2_size, 1])\n",
    "        b3 = bias_variable([1])\n",
    "\n",
    "        # 1st Hidden layer, OPTION: Softmax, relu, tanh or sigmoid\n",
    "        h1 = tf.nn.relu(tf.matmul(state_input, w1) + b1)\n",
    "        # 2nd Hidden layer, OPTION: Softmax, relu, tanh or sigmoid\n",
    "        # Action inserted here\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, w2) + tf.matmul(action_input, w2a) + b2)\n",
    "\n",
    "        reward_value_output = tf.matmul(h2, w3) + b3\n",
    "\n",
    "        return state_input, action_input, reward_value_output, [w1, b1, w2, w2a, b2, w3, b3]\n",
    "\n",
    "    def create_target_reward_network(self, state_dimension, action_dimension, net):\n",
    "        state_input = tf.placeholder(\"float\", [None, state_dimension])\n",
    "        action_input = tf.placeholder(\"float\", [None, action_dimension])\n",
    "\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=1 - CRITIC_TAU)\n",
    "        target_update = ema.apply(net)\n",
    "        target_net = [ema.average(x) for x in net]\n",
    "\n",
    "        layer1 = tf.nn.relu(tf.matmul(state_input, target_net[0]) + target_net[1])\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1, target_net[2]) + tf.matmul(action_input, target_net[3]) + target_net[4])\n",
    "        reward_value_output = tf.identity(tf.matmul(layer2, target_net[5]) + target_net[6])\n",
    "\n",
    "        return state_input, action_input, reward_value_output, target_update\n",
    "\n",
    "    def update_target(self):\n",
    "        self.sess.run(self.target_update)\n",
    "\n",
    "    def train(self, y_batch, state_batch, action_batch):\n",
    "        # r_loss_summ = tf.summary.scalar('reward_critic_loss', self.cost)\n",
    "        # self.merged = tf.summary.merge([r_loss_summ])\n",
    "\n",
    "        train_feed_dict = {\n",
    "            self.y_input: y_batch,\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch\n",
    "        }\n",
    "        _, reward_critic_loss, reward_action_grad_norm = \\\n",
    "            self.sess.run([self.optimizer, self.cost, self.action_gradients], train_feed_dict)\n",
    "\n",
    "        # if self.step % self.log_iter == 0:\n",
    "        #     self.train_writer.add_summary(merged_summ, global_step=self.step)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        return reward_critic_loss, reward_action_grad_norm\n",
    "\n",
    "    def pretrain(self, y_batch, state_batch, action_batch):\n",
    "        train_feed_dict = {\n",
    "            self.y_input: y_batch,\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch\n",
    "        }\n",
    "        _, reward_critic_loss = self.sess.run([self.optimizer, self.cost], train_feed_dict)\n",
    "        return reward_critic_loss\n",
    "\n",
    "    def gradients(self, state_batch, action_batch):\n",
    "        return self.sess.run(self.action_gradients, feed_dict={\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch\n",
    "        })[0]\n",
    "\n",
    "    def target_reward(self, state_batch, action_batch):\n",
    "        return self.sess.run(self.target_reward_value_output, feed_dict={\n",
    "            self.target_state_input: state_batch,\n",
    "            self.target_action_input: action_batch\n",
    "        })\n",
    "\n",
    "    def reward_value(self, state_batch, action_batch):\n",
    "        return self.sess.run(self.reward_value_output, feed_dict={\n",
    "            self.state_input: state_batch,\n",
    "            self.action_input: action_batch})\n",
    "\n",
    "        # f fan-in size\n",
    "    def variable(self, shape, f):\n",
    "        return tf.Variable(tf.random_uniform(shape, -1 / math.sqrt(f), 1 / math.sqrt(f)))\n",
    "\n",
    "'''\n",
    "    def load_network(self):\n",
    "        self.saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.get_checkpoint_state(\"saved_reward_critic_networks\")\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            print \"Successfully loaded:\", checkpoint.model_checkpoint_path\n",
    "        else:\n",
    "            print \"Could not find old network weights\"\n",
    "    def save_network(self,time_step):\n",
    "        print 'save reward-critic-network...',time_step\n",
    "        self.saver.save(self.sess, 'saved_reward_critic_networks/' + 'reward-critic-network', global_step = time_step)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTER_START_POS = 0\n",
    "OUTER_SIZE = 11\n",
    "STATE_SIZE = 47\n",
    "ACTION_SIZE = 51\n",
    "STATE_START_POS = OUTER_START_POS + OUTER_SIZE\n",
    "ACTION_START_POS = STATE_START_POS + STATE_SIZE\n",
    "NEW_STATE_START_POS = ACTION_START_POS + ACTION_SIZE\n",
    "\n",
    "NOX_POS = 40\n",
    "STEAM_TEMP_POS = 48\n",
    "STEAM_PRES_POS = 49\n",
    "NEG_PRES_POS = 44\n",
    "LIM_LOAD_POS = 11\n",
    "LOAD_POS = 46\n",
    "EFFI_WEIGHT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficiency(state):\n",
    "    return 0.8  # @todo\n",
    "\n",
    "def get_emission(state):\n",
    "    return 0.2  # @todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(state):\n",
    "    # coals = get_coals(action)\n",
    "    efficiency = get_efficiency(state)\n",
    "    emission = get_emission(state)\n",
    "    # print('effi', EFFI_WEIGHT * efficiency - (1-EFFI_WEIGHT) * emission)\n",
    "    reward = EFFI_WEIGHT * efficiency - (1-EFFI_WEIGHT) * emission\n",
    "    if np.mean(reward) > 1:\n",
    "        print(reward, efficiency, emission)\n",
    "    return 10*(EFFI_WEIGHT * efficiency - (1-EFFI_WEIGHT) * emission)\n",
    "\n",
    "\n",
    "def compute_cost(state):\n",
    "    # cost 1, 负荷:lim_load ~ limload+25\n",
    "    cost_load = np.zeros([len(state)])\n",
    "\n",
    "    # cost 2, 主蒸汽温度:569-10 ~ 569+5\n",
    "    cost_steam_temp = np.zeros([len(state)])\n",
    "\n",
    "    # cost 3, 主蒸汽压力:given_pres-0.5 ~ given_pres+0.5\n",
    "    cost_steam_pres = np.zeros([len(state)])\n",
    "\n",
    "    return 1/3*cost_load + 1/3*cost_steam_temp + 1/3*cost_steam_pres\n",
    "\n",
    "\n",
    "def compute_done(state):\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Using explorated data based on simulator\"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "        self.real_data = np.load('/Users/xhr/PycharmProjects/Boiler/Simulator/data/replay_buffer.npy')\n",
    "        nums = len(self.real_data)\n",
    "        self.num_indices = list(range(nums))\n",
    "        random.shuffle(self.num_indices)\n",
    "        self.real_start_indice = 0\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # Randomly sample batch_size examples\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def get_real_batch(self, batch_size):\n",
    "        return self.real_data[np.random.choice(self.real_data.shape[0], batch_size, replace=False), :]\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, cost, new_state, done, mix_ratio):\n",
    "        experience = (state, action, reward, cost, new_state, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            for _ in range(mix_ratio):\n",
    "                s, a, s_, done = self.generate_real()\n",
    "                r = compute_reward(s)\n",
    "                c = compute_cost(s)\n",
    "                d = compute_done(s)\n",
    "                e = (s, a, r, c, s_, d)\n",
    "                # print('s-{}-a{}-ns{}'.format(s.shape, a.shape, s_.shape))\n",
    "\n",
    "                self.buffer.append(e)\n",
    "            self.num_experiences += 1\n",
    "        else:\n",
    "            for _ in range(mix_ratio+1):\n",
    "                self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "            for _ in range(mix_ratio):\n",
    "                s, a, s_, done = self.generate_real()\n",
    "                r = compute_reward(s)\n",
    "                c = compute_cost(s)\n",
    "                d = compute_done(s)\n",
    "                e = (s, a, r, c, s_, d)\n",
    "                self.buffer.append(e)\n",
    "\n",
    "    def generate_real(self):\n",
    "            s = self.real_data[self.real_start_indice, :58]\n",
    "            a = self.real_data[self.real_start_indice, 58:109]\n",
    "            s_ = self.real_data[self.real_start_indice, 109:156]\n",
    "            s_ = np.concatenate([s[:11], s_])\n",
    "            done = self.real_data[self.real_start_indice, -1]\n",
    "            self.real_start_indice += 1\n",
    "            if self.real_start_indice == len(self.real_data):\n",
    "                self.real_start_indice = 0\n",
    "            return s, a, s_, done\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"docstring for OUNoise\"\"\"\n",
    "    def __init__(self, action_dimension, mu=0.5, theta=0.4, sigma=0.2, weight_decay=0.9999):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "        self.weight = 1\n",
    "        self.weight_decay = weight_decay\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def update_weight(self):\n",
    "        self.weight *= self.weight_decay\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx * self.weight\n",
    "        return self.state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPSILON定义一个极小值\n",
    "EPSILON = 1e-5\n",
    "# Hyper Parameters:\n",
    "REPLAY_MEMORY_SIZE = 10000\n",
    "REPLAY_START_SIZE = 1000\n",
    "GAMMA = 0.9\n",
    "COST_EPSILON = 1\n",
    "DUAL_STEP_SIZE = 0.01\n",
    "is_grad_inverter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimalDualDDPG(object):\n",
    "    \"\"\" Primal Dual Deep Deterministic Policy Gradient Algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, sess, input_config, is_batch_norm, summ_writer=None, load_model=False):\n",
    "        self.state_dimension = input_config.state_dimension\n",
    "        self.action_dimension = input_config.action_dimension\n",
    "        self.dual_lambda = input_config.init_dual_lambda\n",
    "        self.save_path = input_config.model_save_path\n",
    "        self.train_display_iter = input_config.train_display_iter\n",
    "        self.batch_size = input_config.batch_size\n",
    "        self.gamma = GAMMA\n",
    "        self.summay_writer = summ_writer\n",
    "\n",
    "        self.sess = sess\n",
    "        self.step = 0\n",
    "\n",
    "\n",
    "        # if is_batch_norm:\n",
    "        #     self.rewward_critic_network = RewardCriticNetwork(self.sess, self.state_dimension, self.action_dimension)\n",
    "        #     self.cost_critic_network = CostCriticNetwork(self.sess, self.state_dimension, self.action_dimension)\n",
    "        #     self.actor_network = ActorNetwork(self.sess, self.state_dimension, self.action_dimension)\n",
    "\n",
    "        # else:\n",
    "        self.reward_critic_network = RewardCriticNetwork(self.sess, input_config, self.summay_writer)\n",
    "        self.cost_critic_network = CostCriticNetwork(self.sess, input_config, self.summay_writer)\n",
    "        self.actor_network = ActorNetwork(self.sess, input_config, load_model=False, summ_writer=self.summay_writer)\n",
    "\n",
    "        # initialize replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        # Initialize a random process the Ornstein-Uhlenbeck process for action exploration\n",
    "        self.exploration_noise = OUNoise(self.action_dimension)\n",
    "\n",
    "        # for name in input_config.__dict__:\n",
    "        #     if isinstance(input_config.__dict__[name], int) or isinstance(input_config.__dict__[name], float):\n",
    "        #         self.log(f'parameter|input_config_{name}:{input_config.__dict__[name]}')\n",
    "\n",
    "        # model saver\n",
    "        self.saver = tf.train.Saver()\n",
    "        if load_model:\n",
    "            self.saver.restore(sess=self.sess, save_path=tf.train.latest_checkpoint(self.save_path))\n",
    "\n",
    "\n",
    "    # def __del__(self):\n",
    "    #     self.logfile.close()\n",
    "    #\n",
    "    # def log(self, *args):\n",
    "    #     self.logfile.write(*args)\n",
    "    #     self.logfile.write('\\n')\n",
    "\n",
    "    def train(self):\n",
    "        # print \"train step\", self.time_step\n",
    "        # Sample a random minibatch of N transitions from replay buffer\n",
    "        minibatch = self.replay_buffer.get_batch(self.batch_size)\n",
    "        state_batch = np.asarray([data[0] for data in minibatch])\n",
    "        action_batch = np.asarray([data[1] for data in minibatch])\n",
    "        reward_batch = np.asarray([data[2] for data in minibatch])\n",
    "        cost_batch = np.asarray([data[3] for data in minibatch])\n",
    "        next_state_batch = np.asarray([data[4] for data in minibatch])\n",
    "        done_batch = np.asarray([data[5] for data in minibatch])\n",
    "\n",
    "        # Calculate y_batch\n",
    "        target_action_batch = self.actor_network.target_actions(next_state_batch)\n",
    "        target_reward_value = self.reward_critic_network.target_reward(next_state_batch, target_action_batch)\n",
    "        target_cost_value = self.cost_critic_network.target_cost(next_state_batch, target_action_batch)\n",
    "        y_batch, z_batch = [], []\n",
    "        for i in range(len(minibatch)):\n",
    "            if done_batch[i]:\n",
    "                y_batch.append(reward_batch[i])\n",
    "                z_batch.append(cost_batch[i])\n",
    "            else:\n",
    "                y_batch.append(reward_batch[i] + GAMMA * target_reward_value[i])\n",
    "                z_batch.append(cost_batch[i] + GAMMA * target_cost_value[i])\n",
    "\n",
    "        y_batch = np.resize(y_batch, [self.batch_size, 1])\n",
    "        z_batch = np.resize(z_batch, [self.batch_size, 1])\n",
    "\n",
    "        # Update reward critic by minimizing the loss L\n",
    "        reward_critic_loss, reward_action_grad_norm = self.reward_critic_network.train(y_batch, state_batch, action_batch)\n",
    "        # q_value = self.critic_network.get_q_value(state_limit_batch, action_batch)\n",
    "\n",
    "        # Update cost critic by minimizing the loss L\n",
    "        cost_critic_loss, cost_action_grad_norm = self.cost_critic_network.train(z_batch, state_batch, action_batch)\n",
    "\n",
    "        # Update the actor policy using the sampled gradient\n",
    "        if is_grad_inverter:\n",
    "            action_batch_for_gradients = self.actor_network.actions(state_batch)\n",
    "            action_batch_for_gradients = self.grad_inv.invert(action_batch_for_gradients, )\n",
    "        else:\n",
    "            action_batch_for_gradients = self.actor_network.actions(state_batch)\n",
    "        print('action_batch_for_gradients', action_batch_for_gradients)\n",
    "        reward_gradient_batch = self.reward_critic_network.gradients(state_batch, action_batch_for_gradients)\n",
    "        cost_gradient_batch = self.cost_critic_network.gradients(state_batch, action_batch_for_gradients)\n",
    "        q_gradient_batch = reward_gradient_batch - self.dual_lambda * cost_gradient_batch\n",
    "        self.actor_network.train(q_gradient_batch, state_batch)\n",
    "\n",
    "        # Update the dual variable using the sample gradient\n",
    "        cost_value_batch = self.cost_critic_network.cost_value(state_batch, action_batch_for_gradients)\n",
    "        cost_limit_batch = np.array([[COST_EPSILON] for _ in range(self.batch_size)])\n",
    "        self.dual_gradients = np.mean(cost_value_batch - cost_limit_batch)\n",
    "        self.dual_lambda += DUAL_STEP_SIZE * self.dual_gradients\n",
    "        self.dual_lambda = np.max([EPSILON, self.dual_lambda])  # ensure dual >= 0\n",
    "\n",
    "        if self.step % self.train_display_iter == 0:\n",
    "            print(\"reward_critic: loss:{:.3f} action_grads_norm:{:.3f} \"\n",
    "                  \"| cost_critic: loss:{:.3f} action_grads_norm:{:.3f}\"\n",
    "                  \"| q_gradient:{:.3f}\".format(\n",
    "                reward_critic_loss, np.mean(reward_action_grad_norm),\n",
    "                cost_critic_loss, np.mean(cost_action_grad_norm), np.mean(q_gradient_batch)))\n",
    "            print(\"Dual lambda: {}\".format(self.dual_lambda))\n",
    "\n",
    "\n",
    "        # Update the target networks\n",
    "        self.reward_critic_network.update_target()\n",
    "        self.cost_critic_network.update_target()\n",
    "        self.actor_network.update_target()\n",
    "        self.step += 1\n",
    "\n",
    "    def noise_action(self, state, episode):\n",
    "        # Select action a_t according to the current policy and exploration noise\n",
    "        action = self.actor_network.action(state)\n",
    "        if episode % 10 == 0:\n",
    "            self.exploration_noise.update_weight()\n",
    "        noise_action = action + self.exploration_noise.noise()\n",
    "        noise_action = np.minimum(np.maximum(noise_action, 0), 1)  # bound action to [0, 1]\n",
    "        return noise_action\n",
    "\n",
    "    def action(self, state):\n",
    "        action = self.actor_network.action(state)\n",
    "        return action\n",
    "\n",
    "    def get_dual_lambda(self):\n",
    "        return self.dual_lambda\n",
    "\n",
    "    def perceive(self, state, action, reward, cost, next_state, done, mix_ratio):\n",
    "        # Store transition (s_t,a_t,r_t,c_t,s_{t+1}) in replay buffer\n",
    "        self.replay_buffer.add(state, action, reward, cost, next_state, done, mix_ratio)\n",
    "\n",
    "        # Store transitions to replay start size then start training\n",
    "        if self.replay_buffer.count() > REPLAY_START_SIZE:\n",
    "            self.train()\n",
    "\n",
    "        #if self.time_step % 10000 == 0:\n",
    "            #self.actor_network.save_network(self.time_step)\n",
    "            #self.critic_network.save_network(self.time_step)\n",
    "\n",
    "        # Re-iniitialize the random process when an episode ends\n",
    "        if done:\n",
    "            self.exploration_noise.reset()\n",
    "\n",
    "    def save_model(self):\n",
    "        self.saver.save(sess=self.sess, save_path=self.save_path)  #global_step=10,会自动生成名字-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_REAL_RATIO = 1\n",
    "MAX_EPISODES = 30000\n",
    "MAX_EP_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class input_config():\n",
    "    batch_size = 32\n",
    "    init_dual_lambda = 1\n",
    "    state_dimension = 58\n",
    "    action_dimension = 51\n",
    "    clip_norm = 5.\n",
    "    train_display_iter = 200\n",
    "    model_save_path = './models/'\n",
    "    # model_name = \"sim_ddpg\"\n",
    "    # logdir = './logs/{}-{}-{}-{:.2f}/'.format(\n",
    "    #     model_name, MAX_EP_STEPS, SIM_REAL_RATIO, init_dual_lambda)\n",
    "    # log_path = logdir + 'saved_models/'\n",
    "    log_path = \"logs/nonpre_nonexp_\" + str(SIM_REAL_RATIO) + \"_pdddpg_summary\"\n",
    "    save_iter = 500\n",
    "    log_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train_actor_network(agent, epochs=3):\n",
    "    replay_buffer = agent.replay_buffer\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        step = 0\n",
    "        while step < 1000:\n",
    "            minibatch = replay_buffer.get_real_batch(batch_size=input_config.batch_size)\n",
    "            step += 1\n",
    "            state_batch, action_batch, _, _ = convert_to_tuple(minibatch)\n",
    "\n",
    "            _, mse = agent.actor_network.pretrain(state=state_batch, label=action_batch)\n",
    "\n",
    "        # display\n",
    "        if epoch % 1 == 0:\n",
    "            print('-----------------pre-train actor network-----------------')\n",
    "            print('epoch = {} mse = {:.4f}'.format(epoch, mse))\n",
    "\n",
    "# 预训练 reward critical 网络\n",
    "def pre_train_reward_critic_network(agent, epochs=3):\n",
    "    replay_buffer = agent.replay_buffer\n",
    "    for train_times in range(epochs):\n",
    "        step = 0\n",
    "        while step < 1000:\n",
    "            minibatch = replay_buffer.get_real_batch(batch_size=input_config.batch_size)\n",
    "            step += 1\n",
    "            state_batch, action_batch, next_state_batch, _ = convert_to_tuple(minibatch)\n",
    "            reward_batch = compute_reward(state_batch)\n",
    "\n",
    "            y_batch = []\n",
    "            target_action = agent.actor_network.target_actions(next_state_batch)\n",
    "            target_value = agent.reward_critic_network.target_reward(next_state_batch, target_action)\n",
    "\n",
    "            for i in range(len(minibatch)):\n",
    "                y_batch.append(reward_batch[i] + agent.gamma * target_value[i])\n",
    "\n",
    "            # update critic network\n",
    "            reward_critic_loss = agent.reward_critic_network.pretrain(y_batch, state_batch, action_batch)\n",
    "\n",
    "        # display\n",
    "        if train_times % 1 == 0:\n",
    "            print('-----------------pre-train reward critic network-----------------')\n",
    "            print(\"reward_critic: loss:{:.3f}\".format(reward_critic_loss))\n",
    "\n",
    "# 预训练 cost critical 网络\n",
    "def pre_train_cost_critic_network(agent, epochs=3):\n",
    "    replay_buffer = agent.replay_buffer\n",
    "    step = 0\n",
    "    for train_times in range(epochs):\n",
    "        step = 0\n",
    "        while step < 1000:\n",
    "            minibatch = replay_buffer.get_real_batch(batch_size=input_config.batch_size)\n",
    "            step += 1\n",
    "            state_batch, action_batch, next_state_batch, _ = convert_to_tuple(minibatch)\n",
    "            cost_batch = compute_cost(state_batch)\n",
    "\n",
    "            z_batch = []\n",
    "            target_action = agent.actor_network.target_actions(next_state_batch)\n",
    "            target_value = agent.cost_critic_network.target_cost(next_state_batch, target_action)\n",
    "\n",
    "            for i in range(len(minibatch)):\n",
    "                z_batch.append(cost_batch[i] + agent.gamma * target_value[i])\n",
    "\n",
    "            # update critic network\n",
    "            cost_critic_loss = agent.cost_critic_network.pretrain(z_batch, state_batch, action_batch)\n",
    "\n",
    "        # display\n",
    "        if train_times % 1 == 0:\n",
    "            print('-----------------pre-train cost critic network-----------------')\n",
    "            print(\"reward_critic: loss:{:.3f}\".format(cost_critic_loss))\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    # Set up summary writer\n",
    "    summary_writer = tf.summary.FileWriter(input_config.log_path)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    agent_graph = tf.Graph()\n",
    "    agent_sess = tf.Session(config=config, graph=agent_graph)\n",
    "    with agent_graph.as_default():\n",
    "        agent = PrimalDualDDPG(sess=agent_sess, input_config=input_config, is_batch_norm=False, summ_writer=summary_writer)\n",
    "        total_parameters = 0\n",
    "        for variable in tf.trainable_variables():\n",
    "            # shape is an array of tf.Dimension\n",
    "            shape = variable.get_shape()\n",
    "            # print(shape)\n",
    "            # print(len(shape))\n",
    "            variable_parameters = 1\n",
    "            for dim in shape:\n",
    "                # print(dim)\n",
    "                variable_parameters *= dim.value\n",
    "            # print(variable_parameters)\n",
    "            total_parameters += variable_parameters\n",
    "        print('total parameters: {}'.format(total_parameters))\n",
    "\n",
    "    # build environment graph\n",
    "    env_graph = tf.Graph()\n",
    "    env_sess = tf.Session(config=config, graph=env_graph)\n",
    "    with env_graph.as_default():\n",
    "        env = SimulatorEnvironment(sess=env_sess)\n",
    "\n",
    "    # pre_train\n",
    "    # pre_train_actor_network(agent=agent, epochs=1)\n",
    "    # pre_train_reward_critic_network(agent=agent, epochs=1)\n",
    "    # pre_train_cost_critic_network(agent=agent, epochs=1)\n",
    "    # agent.actor_network.update_target()\n",
    "    # agent.reward_critic_network.update_target()\n",
    "    # agent.cost_critic_network.update_target()\n",
    "\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        dual_variable = input_config.init_dual_lambda\n",
    "        ep_reward = 0\n",
    "        ep_cost = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(MAX_EP_STEPS):\n",
    "            # action = restrictive_action(agent.action(state), episode)\n",
    "            action = agent.noise_action(state, episode)\n",
    "            next_state, reward, cost, done = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_cost += cost\n",
    "            agent.perceive(state, action, reward, cost, next_state, done, mix_ratio=SIM_REAL_RATIO)\n",
    "            dual_variable = agent.get_dual_lambda()\n",
    "            state = next_state\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='Steps_sum_Reward', simple_value=float(ep_reward/MAX_EP_STEPS))\n",
    "        summary.value.add(tag='Steps_sum_Cost', simple_value=float(ep_cost/MAX_EP_STEPS))\n",
    "        summary.value.add(tag='Dual_variable', simple_value=float(dual_variable))\n",
    "        summary_writer.add_summary(summary, episode)\n",
    "\n",
    "        summary_writer.flush()\n",
    "\n",
    "        print('Episode:{} | Reward: {:.2f} | Cost: {:.2f}'.format(episode, ep_reward/MAX_EP_STEPS, ep_cost/MAX_EP_STEPS))\n",
    "\n",
    "        if episode % 100 == 0 and episode >= 100:\n",
    "            agent.save_model()\n",
    "\n",
    "    print(\"-------------save model--------------------\")\n",
    "    agent.save_model()\n",
    "\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_variable() missing 1 required positional argument: 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\TPGU\\RL\\RL.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main()\n",
      "\u001b[1;32me:\\TPGU\\RL\\RL.ipynb Cell 21\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m agent_sess \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mSession(config\u001b[39m=\u001b[39mconfig, graph\u001b[39m=\u001b[39magent_graph)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mwith\u001b[39;00m agent_graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     agent \u001b[39m=\u001b[39m PrimalDualDDPG(sess\u001b[39m=\u001b[39;49magent_sess, input_config\u001b[39m=\u001b[39;49minput_config, is_batch_norm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, summ_writer\u001b[39m=\u001b[39;49msummary_writer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     total_parameters \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mtrainable_variables():\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m         \u001b[39m# shape is an array of tf.Dimension\u001b[39;00m\n",
      "\u001b[1;32me:\\TPGU\\RL\\RL.ipynb Cell 21\u001b[0m in \u001b[0;36mPrimalDualDDPG.__init__\u001b[1;34m(self, sess, input_config, is_batch_norm, summ_writer, load_model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# if is_batch_norm:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#     self.rewward_critic_network = RewardCriticNetwork(self.sess, self.state_dimension, self.action_dimension)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#     self.cost_critic_network = CostCriticNetwork(self.sess, self.state_dimension, self.action_dimension)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#     self.actor_network = ActorNetwork(self.sess, self.state_dimension, self.action_dimension)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_critic_network \u001b[39m=\u001b[39m RewardCriticNetwork(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msess, input_config, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msummay_writer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_critic_network \u001b[39m=\u001b[39m CostCriticNetwork(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msess, input_config, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummay_writer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_network \u001b[39m=\u001b[39m ActorNetwork(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msess, input_config, load_model\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, summ_writer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummay_writer)\n",
      "\u001b[1;32me:\\TPGU\\RL\\RL.ipynb Cell 21\u001b[0m in \u001b[0;36mRewardCriticNetwork.__init__\u001b[1;34m(self, sess, input_config, summ_writer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_writer \u001b[39m=\u001b[39m summ_writer\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# create reward network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_input, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_input, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_value_output, \\\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_reward_network(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate_dimension, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_dimension)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# create target reward network (the same structure with reward network)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_state_input, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_action_input, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_reward_value_output, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_update \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_target_reward_network(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_dimension, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dimension, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet)\n",
      "\u001b[1;32me:\\TPGU\\RL\\RL.ipynb Cell 21\u001b[0m in \u001b[0;36mRewardCriticNetwork.create_reward_network\u001b[1;34m(self, state_dimension, action_dimension)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m action_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mplaceholder(\u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39mNone\u001b[39;00m, action_dimension])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Input -> Hidden Layer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m w1 \u001b[39m=\u001b[39m weight_variable([state_dimension, layer1_size])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m b1 \u001b[39m=\u001b[39m bias_variable([layer1_size])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/TPGU/RL/RL.ipynb#X26sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# Hidden Layer -> Hidden Layer + Action\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_variable() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
