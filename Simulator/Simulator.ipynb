{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoilerDataSet(object):\n",
    "    \"\"\"\n",
    "    first run data_preparation.py to generate data.csv\n",
    "    prepare boiler training and validation dataset\n",
    "    simple version(small action dimension)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_steps, val_ratio=0.1):\n",
    "        self.num_steps = num_steps  # time steps\n",
    "        self.val_ratio = val_ratio  # train test ratio\n",
    "        \n",
    "        # Read csv file\n",
    "        self.raw_data = pd.read_csv(\"./data/sim_train.csv\", index_col='时间戳')\n",
    "\n",
    "        # sort csv file\n",
    "        cols = self.raw_data.columns.tolist()\n",
    "        # print(\"origin len: {0}\".format(len(cols)))\n",
    "        cols = (cols[51:52] + cols[53:59] + cols [60:61] + cols[62:63] + cols[150:152]   # external input \n",
    "            + cols[0:50] + cols[52:53] + cols[122:139]  # Coal Pulverizing state\n",
    "            + cols[50:51] + cols[59:60] + cols[61:62] + cols[63:101] + cols[112:114] + cols[118:122] + cols[139:145] + cols[146:149] + cols[152:158]    # Burning state\n",
    "            + cols[101:112] + cols[114:118] + cols[145:146] + cols[149:150] # Steam Circulation state\n",
    "            + cols[158:173] + cols[196:202] # Coal Pulverizing action\n",
    "            + cols[173:192]                 # Burning action\n",
    "            + cols[192:196])                # Steam Circulation action\n",
    "        # print(\"ordered len: {0}\".format(len(cols)))\n",
    "        self.raw_data = self.raw_data[cols]\n",
    "\n",
    "        # divide train set and valid set\n",
    "        self.train_X, self.train_y, self.valid_X, self.valid_y = self.prepare_data(self.raw_data)\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        # split into groups of num_steps\n",
    "\n",
    "        # 取出输入数据，学习num_steps步长的历史，iloc：通过行号获取行数据\n",
    "        X = np.array([data.iloc[i: i + self.num_steps].values\n",
    "                    for i in range(len(data) - self.num_steps)])\n",
    "\n",
    "        # 取出输出数据，预测第num_steps步的值训练，ix / loc 可以通过行号和行标签进行索引\n",
    "        # 这里只要对状态量进行预测即可，0-157列为 'A磨煤机电流':'大渣可燃物含量'\n",
    "        y = np.array([data.iloc[i + self.num_steps, 0:158].values\n",
    "                    for i in range(len(data) - self.num_steps)])\n",
    "\n",
    "        train_size = int(len(X) * (1.0 - self.val_ratio))\n",
    "        train_X, valid_X = X[:train_size], X[train_size:]\n",
    "        train_y, valid_y = y[:train_size], y[train_size:]\n",
    "        return train_X, train_y, valid_X, valid_y\n",
    "\n",
    "    def generate_one_epoch(self, data_X, data_y, batch_size):\n",
    "        num_batches = int(len(data_X)) // batch_size\n",
    "        # if batch_size * num_batches < len(self.train_X):\n",
    "        #     num_batches += 1\n",
    "\n",
    "        batch_indices = list(range(num_batches))\n",
    "        random.shuffle(batch_indices)\n",
    "        for j in batch_indices:\n",
    "            batch_X = data_X[j * batch_size: (j + 1) * batch_size]\n",
    "            batch_y = data_y[j * batch_size: (j + 1) * batch_size]\n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class sim_config(object):\n",
    "num_steps = 10\n",
    "valid_ratio = 0.2\n",
    "\n",
    "input_size = 202\n",
    "num_neurons = 160\n",
    "num_layers = 3\n",
    "output_size = 158\n",
    "\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.95\n",
    "\n",
    "max_epoch = 50\n",
    "batch_size = 1\n",
    "\n",
    "save_log_iter = 10\n",
    "display_iter = 20\n",
    "\n",
    "tf.random.set_seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatorRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.rnn_cell = tf.keras.layers.LSTMCell(units)\n",
    "                        # tf.keras.layers.SimpleRNNCell(units, # LSTMCell(units,\n",
    "                        #                                activation=None)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatorRNNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, cell, return_sequences=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cell = cell\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "    def get_initial_state(self, inputs):\n",
    "        try:\n",
    "            return self.cell.get_initial_state(inputs)\n",
    "        except AttributeError:\n",
    "            # fallback to zeros if self.cell has no get_initial_state() method\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            return [tf.zeros([batch_size, self.cell.state_size],\n",
    "                             dtype=inputs.dtype)]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        states = self.get_initial_state(inputs)\n",
    "        shape = tf.shape(inputs)\n",
    "        batch_size = shape[0]\n",
    "        n_steps = shape[1]\n",
    "        sequences = tf.TensorArray(\n",
    "            inputs.dtype, size=(n_steps if self.return_sequences else 0))\n",
    "        outputs = tf.zeros(shape=[batch_size, self.cell.output_size],\n",
    "                           dtype=inputs.dtype)\n",
    "        for step in tf.range(n_steps):\n",
    "            outputs, states = self.cell(inputs[:, step], states)\n",
    "            if self.return_sequences:\n",
    "                sequences = sequences.write(step, outputs)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            # stack the outputs into an array of shape\n",
    "            # [time steps, batch size, dims], then transpose it to shape\n",
    "            # [batch size, time steps, dims]\n",
    "            return tf.transpose(sequences.stack(), [1, 0, 2])\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 256)           470016    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " targets (Dense)             (None, 158)               20382     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 687,518\n",
      "Trainable params: 687,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def SimulatorRNNModel():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=[num_steps, input_size], name=\"inputs\"),\n",
    "        tf.keras.layers.LSTM(256, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        # tf.keras.layers.RNN(SimulatorRNNCell(128), input_shape=[None, num_steps, input_size]),\n",
    "        tf.keras.layers.Dense(158, name=\"targets\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(), \n",
    "        optimizer=tf.keras.optimizers.Nadam(learning_rate=learning_rate), \n",
    "        metrics=['mean_absolute_error']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = SimulatorRNNModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model, train_X, train_y, valid_X, valid_y, learning_rate, batch_size=1, epochs=500):\n",
    "    callback_list = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            patience=50, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath='./logs/LSTM/saved_models/model-{epoch:02d}-{val_loss:.4f}.h5',\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=1,\n",
    "            save_weights_only=True,\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs/')\n",
    "    ]   \n",
    "    \n",
    "    history = model.fit(\n",
    "        x=train_X, y=train_y,\n",
    "        epochs=epochs, \n",
    "        validation_data=(valid_X, valid_y),\n",
    "        callbacks=callback_list)\n",
    "    valid_loss, valid_mae = model.evaluate(x=valid_X, y=valid_y) # Returns the loss value & metrics values for the model in test mode\n",
    "    return valid_mae * 1e6  # valid mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.7237e-04 - mean_absolute_error: 0.0210\n",
      "Epoch 1: val_loss improved from inf to 0.00219, saving model to ./logs/LSTM/saved_models\\model-01-0.0022.h5\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 7.7237e-04 - mean_absolute_error: 0.0210 - val_loss: 0.0022 - val_mean_absolute_error: 0.0271\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.1251e-04 - mean_absolute_error: 0.0201\n",
      "Epoch 2: val_loss did not improve from 0.00219\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 7.1251e-04 - mean_absolute_error: 0.0201 - val_loss: 0.0025 - val_mean_absolute_error: 0.0299\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.8528e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 3: val_loss improved from 0.00219 to 0.00213, saving model to ./logs/LSTM/saved_models\\model-03-0.0021.h5\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 6.8528e-04 - mean_absolute_error: 0.0197 - val_loss: 0.0021 - val_mean_absolute_error: 0.0262\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.4545e-04 - mean_absolute_error: 0.0190\n",
      "Epoch 4: val_loss did not improve from 0.00213\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 6.4545e-04 - mean_absolute_error: 0.0190 - val_loss: 0.0024 - val_mean_absolute_error: 0.0291\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.3389e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 5: val_loss improved from 0.00213 to 0.00211, saving model to ./logs/LSTM/saved_models\\model-05-0.0021.h5\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 6.3389e-04 - mean_absolute_error: 0.0189 - val_loss: 0.0021 - val_mean_absolute_error: 0.0259\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.1671e-04 - mean_absolute_error: 0.0185\n",
      "Epoch 6: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 6.1671e-04 - mean_absolute_error: 0.0185 - val_loss: 0.0024 - val_mean_absolute_error: 0.0290\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.2401e-04 - mean_absolute_error: 0.0189\n",
      "Epoch 7: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 6.2401e-04 - mean_absolute_error: 0.0189 - val_loss: 0.0021 - val_mean_absolute_error: 0.0263\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.2935e-04 - mean_absolute_error: 0.0188\n",
      "Epoch 8: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 6.2935e-04 - mean_absolute_error: 0.0188 - val_loss: 0.0024 - val_mean_absolute_error: 0.0296\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.5731e-04 - mean_absolute_error: 0.0197\n",
      "Epoch 9: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.5731e-04 - mean_absolute_error: 0.0197 - val_loss: 0.0022 - val_mean_absolute_error: 0.0273\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.8302e-04 - mean_absolute_error: 0.0198\n",
      "Epoch 10: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 6.8302e-04 - mean_absolute_error: 0.0198 - val_loss: 0.0025 - val_mean_absolute_error: 0.0307\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.2970e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 11: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.2970e-04 - mean_absolute_error: 0.0211 - val_loss: 0.0023 - val_mean_absolute_error: 0.0288\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.6725e-04 - mean_absolute_error: 0.0213\n",
      "Epoch 12: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 7.6725e-04 - mean_absolute_error: 0.0213 - val_loss: 0.0026 - val_mean_absolute_error: 0.0320\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.2102e-04 - mean_absolute_error: 0.0228\n",
      "Epoch 13: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.2102e-04 - mean_absolute_error: 0.0228 - val_loss: 0.0024 - val_mean_absolute_error: 0.0302\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.5136e-04 - mean_absolute_error: 0.0227\n",
      "Epoch 14: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 8.5136e-04 - mean_absolute_error: 0.0227 - val_loss: 0.0026 - val_mean_absolute_error: 0.0330\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.8964e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 15: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 8.8964e-04 - mean_absolute_error: 0.0239 - val_loss: 0.0024 - val_mean_absolute_error: 0.0309\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.9089e-04 - mean_absolute_error: 0.0232\n",
      "Epoch 16: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 8.9089e-04 - mean_absolute_error: 0.0232 - val_loss: 0.0026 - val_mean_absolute_error: 0.0331\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.9370e-04 - mean_absolute_error: 0.0239\n",
      "Epoch 17: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 8.9370e-04 - mean_absolute_error: 0.0239 - val_loss: 0.0024 - val_mean_absolute_error: 0.0305\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.5947e-04 - mean_absolute_error: 0.0228\n",
      "Epoch 18: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 8.5947e-04 - mean_absolute_error: 0.0228 - val_loss: 0.0026 - val_mean_absolute_error: 0.0322\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 8.2728e-04 - mean_absolute_error: 0.0228\n",
      "Epoch 19: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 8.2728e-04 - mean_absolute_error: 0.0228 - val_loss: 0.0023 - val_mean_absolute_error: 0.0293\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.7252e-04 - mean_absolute_error: 0.0214\n",
      "Epoch 20: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 7.7252e-04 - mean_absolute_error: 0.0214 - val_loss: 0.0025 - val_mean_absolute_error: 0.0308\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.2414e-04 - mean_absolute_error: 0.0211\n",
      "Epoch 21: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 7.2414e-04 - mean_absolute_error: 0.0211 - val_loss: 0.0022 - val_mean_absolute_error: 0.0277\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.6969e-04 - mean_absolute_error: 0.0196\n",
      "Epoch 22: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 6.6969e-04 - mean_absolute_error: 0.0196 - val_loss: 0.0023 - val_mean_absolute_error: 0.0292\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.2390e-04 - mean_absolute_error: 0.0191\n",
      "Epoch 23: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 6.2390e-04 - mean_absolute_error: 0.0191 - val_loss: 0.0022 - val_mean_absolute_error: 0.0264\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.8162e-04 - mean_absolute_error: 0.0179\n",
      "Epoch 24: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 5.8162e-04 - mean_absolute_error: 0.0179 - val_loss: 0.0023 - val_mean_absolute_error: 0.0277\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.4701e-04 - mean_absolute_error: 0.0175\n",
      "Epoch 25: val_loss improved from 0.00211 to 0.00211, saving model to ./logs/LSTM/saved_models\\model-25-0.0021.h5\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 5.4701e-04 - mean_absolute_error: 0.0175 - val_loss: 0.0021 - val_mean_absolute_error: 0.0253\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.1890e-04 - mean_absolute_error: 0.0165\n",
      "Epoch 26: val_loss did not improve from 0.00211\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 5.1890e-04 - mean_absolute_error: 0.0165 - val_loss: 0.0022 - val_mean_absolute_error: 0.0266\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.9603e-04 - mean_absolute_error: 0.0163\n",
      "Epoch 27: val_loss improved from 0.00211 to 0.00208, saving model to ./logs/LSTM/saved_models\\model-27-0.0021.h5\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 4.9603e-04 - mean_absolute_error: 0.0163 - val_loss: 0.0021 - val_mean_absolute_error: 0.0246\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7952e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 28: val_loss did not improve from 0.00208\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.7952e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0022 - val_mean_absolute_error: 0.0259\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.6592e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 29: val_loss improved from 0.00208 to 0.00206, saving model to ./logs/LSTM/saved_models\\model-29-0.0021.h5\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 4.6592e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0021 - val_mean_absolute_error: 0.0243\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.5774e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 30: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.5774e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0021 - val_mean_absolute_error: 0.0255\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.5070e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 31: val_loss improved from 0.00206 to 0.00206, saving model to ./logs/LSTM/saved_models\\model-31-0.0021.h5\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 4.5070e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0021 - val_mean_absolute_error: 0.0241\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4831e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 32: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.4831e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0021 - val_mean_absolute_error: 0.0253\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4571e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 33: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.4571e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0021 - val_mean_absolute_error: 0.0242\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4739e-04 - mean_absolute_error: 0.0148\n",
      "Epoch 34: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 4.4739e-04 - mean_absolute_error: 0.0148 - val_loss: 0.0021 - val_mean_absolute_error: 0.0252\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4770e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 35: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.4770e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0021 - val_mean_absolute_error: 0.0243\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.5222e-04 - mean_absolute_error: 0.0149\n",
      "Epoch 36: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.5222e-04 - mean_absolute_error: 0.0149 - val_loss: 0.0021 - val_mean_absolute_error: 0.0252\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.5422e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 37: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 4.5422e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0021 - val_mean_absolute_error: 0.0245\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.6056e-04 - mean_absolute_error: 0.0151\n",
      "Epoch 38: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 4.6056e-04 - mean_absolute_error: 0.0151 - val_loss: 0.0021 - val_mean_absolute_error: 0.0253\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.6315e-04 - mean_absolute_error: 0.0153\n",
      "Epoch 39: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 4.6315e-04 - mean_absolute_error: 0.0153 - val_loss: 0.0021 - val_mean_absolute_error: 0.0247\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7027e-04 - mean_absolute_error: 0.0154\n",
      "Epoch 40: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.7027e-04 - mean_absolute_error: 0.0154 - val_loss: 0.0021 - val_mean_absolute_error: 0.0253\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7238e-04 - mean_absolute_error: 0.0155\n",
      "Epoch 41: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 4.7238e-04 - mean_absolute_error: 0.0155 - val_loss: 0.0021 - val_mean_absolute_error: 0.0249\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7917e-04 - mean_absolute_error: 0.0156\n",
      "Epoch 42: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 4.7917e-04 - mean_absolute_error: 0.0156 - val_loss: 0.0021 - val_mean_absolute_error: 0.0254\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7984e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 43: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 4.7984e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0021 - val_mean_absolute_error: 0.0251\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8528e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 44: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 4.8528e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0021 - val_mean_absolute_error: 0.0254\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8385e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 45: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 4.8385e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0252\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8721e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 46: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 4.8721e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0253\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8352e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 47: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 4.8352e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0252\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.8464e-04 - mean_absolute_error: 0.0158\n",
      "Epoch 48: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 4.8464e-04 - mean_absolute_error: 0.0158 - val_loss: 0.0021 - val_mean_absolute_error: 0.0252\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7909e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 49: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 4.7909e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0022 - val_mean_absolute_error: 0.0252\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.7841e-04 - mean_absolute_error: 0.0157\n",
      "Epoch 50: val_loss did not improve from 0.00206\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.7841e-04 - mean_absolute_error: 0.0157 - val_loss: 0.0021 - val_mean_absolute_error: 0.0251\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0021 - mean_absolute_error: 0.0251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25075.042620301247"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "boiler_dataset = BoilerDataSet(num_steps=num_steps, val_ratio=valid_ratio)\n",
    "train_X, train_y = boiler_dataset.train_X, boiler_dataset.train_y\n",
    "valid_X, valid_y = boiler_dataset.valid_X, boiler_dataset.valid_y\n",
    "\n",
    "fit_and_evaluate(model, train_X, train_y, valid_X, valid_y, learning_rate, batch_size, max_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
